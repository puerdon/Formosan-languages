{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f19370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /opt/anaconda3/envs/thai/lib/python3.12/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/envs/thai/lib/python3.12/site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/envs/thai/lib/python3.12/site-packages (from python-docx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d5ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.6/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.4/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 298.0/298.0 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB ? eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/267.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 267.8/267.8 kB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\user\\desktop\\don\\陳思瑋\\南島語料庫\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\desktop\\don\\陳思瑋\\南島語料庫\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d751ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from docx import Document\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20220302.pkl')\n",
    "  df_2 = df.astype(str, errors='ignore')\n",
    "  df_2 = df_2.applymap(lambda x: x[1:] if x.startswith(\".\") else x)\n",
    "  df_2 = df_2.applymap(lambda x: x.strip())\n",
    "  filt = df_2.Ch.apply(len) < 5\n",
    "  df_2 = df_2[~filt]\n",
    "  return df, df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ea49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_2 = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[data['Lang_Ch'].str.contains(\"泰雅\")]\n",
    "x['Lang_Ch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46121682",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = data_2.copy()\n",
    "\n",
    "def read_a_csv_and_drop_na(path):\n",
    "    df = pd.read_csv(path, encoding=\"big5\").dropna(how='all').dropna(axis=1, how='all')\n",
    "    df['Lang_En'] = 'Atayal'\n",
    "    df['Lang_Ch'] = '泰雅_賽考利克'\n",
    "    df['From'] = \"傳說故事精選篇\"\n",
    "    df = df.rename(columns={\"Atayal\": \"Ab\", \"Mandarin\": \"Ch\"})\n",
    "    \n",
    "    return df\n",
    "\n",
    "for x in pathlib.Path(\"../atayal_data/\").glob(\"*.csv\"):\n",
    "    c = read_a_csv_and_drop_na(x)\n",
    "    r = pd.concat([r, c], ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.columns\n",
    "r.drop('Unnamed: 2', inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4127528",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_pickle('./Formosan-Mandarin_sent_pairs_20220106.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./Formosan-Mandarin_sent_pairs_20220106.pkl', compression=\"gzip\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f7f00",
   "metadata": {},
   "source": [
    "# 02-16 更新:只要泰雅語就好, 將 Rau(1992) 資料加進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20220106.pkl', compression=\"gzip\")\n",
    "\n",
    "df_atayal = df[df['Lang_En'] == 'Atayal']\n",
    "print(df_atayal.shape)\n",
    "\n",
    "r = df_atayal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372021a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_1992 = pd.read_excel(\"data/Rau_1992_text_story.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c106d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet in rau_1992:\n",
    "    rau_1992[sheet] = rau_1992[sheet].dropna()\n",
    "    print(\"shape of sheet\", sheet, rau_1992[sheet].shape)\n",
    "    rau_1992[sheet] = rau_1992[sheet].rename(columns={\"Atayal\": \"Ab\", \"English\": \"Ch\"})\n",
    "    rau_1992[sheet]['Lang_En'] = 'Atayal'\n",
    "    rau_1992[sheet]['Lang_Ch'] = '泰雅_賽考利克'\n",
    "    rau_1992[sheet]['From'] = 'Rau (1992)'\n",
    "    \n",
    "    r = pd.concat([r, rau_1992[sheet]], ignore_index=True)\n",
    "    print(r.shape)\n",
    "    print('======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa044",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_pickle('./data/Formosan-Mandarin_sent_pairs_20220216.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef03604",
   "metadata": {},
   "source": [
    "# 02-17 更新: 整理大崁崁群的word檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82358968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def 檢查各檔案的每個句子的tag(path):\n",
    "\n",
    "    doc = Document(path)\n",
    "\n",
    "    having_five_tags = False\n",
    "    after_title_part = False\n",
    "    after_new_line = False\n",
    "\n",
    "    all_sentences = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        \n",
    "#         print(f'{para.text}\\n###')\n",
    "        \n",
    "        ################################\n",
    "        # BEGIN: 處理語料前面的metadata #       \n",
    "        ################################\n",
    "        \n",
    "        if not after_title_part:\n",
    "            if not para.text.startswith(\"A:\") and\\\n",
    "            not para.text.startswith(\"RA:\") and\\\n",
    "            not para.text.startswith(\"G:\") and\\\n",
    "            not para.text.startswith(\"M:\") and\\\n",
    "            not para.text.startswith(\"RM:\"):\n",
    "                continue\n",
    "\n",
    "        after_title_part = True\n",
    "        \n",
    "        \n",
    "\n",
    "        # 遇到換行\n",
    "        if len(para.text.strip()) == 0:\n",
    "            after_new_line = True\n",
    "            all_sentences.append(dict())\n",
    "\n",
    "        if not para.text.startswith(\"A:\") and\\\n",
    "        not para.text.startswith(\"RA:\") and\\\n",
    "        not para.text.startswith(\"G:\") and\\\n",
    "        not para.text.startswith(\"M:\") and\\\n",
    "        not para.text.startswith(\"RM:\"):\n",
    "            continue\n",
    "\n",
    "        for p in para.text.split('\\n'):\n",
    "            if p.startswith(\"A:\") or p.startswith(\"RA:\") or p.startswith(\"G:\") or p.startswith(\"M:\") or p.startswith(\"RM:\"):\n",
    "                tag = p.split(\":\")[0]\n",
    "                content = p.lstrip(tag + \":\").strip()\n",
    "                if content == 'none':\n",
    "                    content = None\n",
    "\n",
    "                if len(all_sentences) == 0:\n",
    "                    all_sentences.append(dict())\n",
    "\n",
    "                all_sentences[-1][tag] = content\n",
    "\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e47d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/泰雅族大嵙崁群的部落故事\").glob(\"*aligned.docx\"):\n",
    "    print(f\"[[[{x}]]]\")\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "\n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        if 'RA' not in row:\n",
    "            ab = row['A']\n",
    "        else:\n",
    "            if row['RA'] is None:\n",
    "                ab = row['A']\n",
    "            else:\n",
    "                ab = row['RA']\n",
    "\n",
    "        if 'RM' not in row:\n",
    "            ch = row['M']\n",
    "        else:\n",
    "            if row['RM'] is None:\n",
    "                ch = row['M']\n",
    "            else:  \n",
    "                ch = row['RM']\n",
    "        \n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "            \n",
    "        results = results.append({'Ab':ab, 'Ch':ch, 'Source': x.name[:4] + \"_\" + str(index)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "results = results.dropna()\n",
    "results['From'] = '大嵙崁群故事'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "display(results.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Source'] == 'TM01_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ea1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20220216.pkl', compression=\"gzip\")\n",
    "\n",
    "# df_atayal = df[df['Lang_En'] == 'Atayal']\n",
    "# print(df_atayal.shape)\n",
    "\n",
    "r = df.append(results, ignore_index=True)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69260918",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_pickle('./data/Formosan-Mandarin_sent_pairs_20220217.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ece85",
   "metadata": {},
   "source": [
    "# Huang 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20220217.pkl', compression=\"gzip\")\n",
    "doc = Document(\"data/Huang_1994_text_aligned.docx\")\n",
    "    \n",
    "results = []\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    print(para.text)\n",
    "    \n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip()\n",
    "        results.append({\"Ab\": text})\n",
    "    elif para.text.startswith(\"e:\"):\n",
    "        text = para.text.lstrip(\"e:\").strip()\n",
    "        results[-1][\"Ch\"] = text\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Huang 1994'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = df.append(results, ignore_index=True)\n",
    "df\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20220302.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fe682",
   "metadata": {},
   "source": [
    "# 加進完整的大嵙崁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48be79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None, 'Source': None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/泰雅族大嵙崁群的部落故事\").glob(\"*aligned.docx\"):\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "#     display(all_sentences)\n",
    "    \n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        try:\n",
    "            ab = row['A'] if row['RA'] is None else row['RA']\n",
    "            ch = row['M'] if row['RM'] is None else row['RM']\n",
    "        except Exception as e:\n",
    "            print(x)\n",
    "            print(e)\n",
    "\n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "        \n",
    "        df= pd.DataFrame({'Ab':[ab], 'Ch':[ch], 'Source': [f'{x.name[:4]}_{str(index)}']})\n",
    "    \n",
    "        results = pd.concat([results, df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "results = results.dropna()\n",
    "results['From'] = '大嵙崁群故事'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "display(results.dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95147bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20220302.pkl', compression=\"gzip\")\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221102.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c8322",
   "metadata": {},
   "source": [
    "# 加進復興鄉故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06002388",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None, 'Source': None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/復興鄉故事\").glob(\"*aligned.docx\"):\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "    all_sentences = all_sentences.replace({np.nan: None})\n",
    "    display(all_sentences)\n",
    "    \n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        try:\n",
    "            ab = row['A'] if row['RA'] is None or len(row['RA']) == 0 else row['RA']\n",
    "            ch = row['M'] if row['RM'] is None or len(row['RM']) == 0 else row['RM']\n",
    "        except Exception as e:\n",
    "            print(x)\n",
    "            print(e)\n",
    "\n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "        \n",
    "        df= pd.DataFrame({'Ab':[ab], 'Ch':[ch], 'Source': [f'{x.name[:4]}_{str(index)}']})\n",
    "    \n",
    "        results = pd.concat([results, df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['From'] = '復興鄉故事'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221102.pkl', compression=\"gzip\")\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221115.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221117.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5baec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df[df['Ab'].str.contains(\"none\", na=False)]['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "\n",
    "    if isinstance(row['Ab'], str) and 'G:' in row['Ab']:\n",
    "        print(row['Ab'])\n",
    "        print('@')\n",
    "\n",
    "        if row['Ab'].startswith('none'):\n",
    "            new = row['Ab'].replace('none\\nG: ', '').strip()\n",
    "        else:\n",
    "            new = re.sub('G:.*','', row['Ab']).strip()\n",
    "        df.at[i,'Ab'] = new\n",
    "        print(new)\n",
    "        print('----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36bf51",
   "metadata": {},
   "source": [
    "# 重新整理復興鄉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ad68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221117.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4904c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecca93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['From'].str.contains('復興鄉')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0165e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None, 'Source': None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/復興鄉故事1\").glob(\"*aligned.docx\"):\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "    all_sentences = all_sentences.replace({np.nan: None})\n",
    "#     display(all_sentences)\n",
    "    \n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        try:\n",
    "            ab = row['A'] if row['RA'] is None or len(row['RA']) == 0 else row['RA']\n",
    "            ch = row['M'] if row['RM'] is None or len(row['RM']) == 0 else row['RM']\n",
    "        except Exception as e:\n",
    "            print(x)\n",
    "            print(e)\n",
    "\n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "        \n",
    "        dff= pd.DataFrame({'Ab':[ab], 'Ch':[ch], 'Source': [f'{x.name[:4]}_{str(index)}']})\n",
    "    \n",
    "        results = pd.concat([results, dff], ignore_index = True)\n",
    "\n",
    "results['From'] = '復興鄉故事一'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "df = pd.concat([df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c001dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None, 'Source': None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/復興鄉故事2\").glob(\"*aligned.docx\"):\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "    all_sentences = all_sentences.replace({np.nan: None})\n",
    "#     display(all_sentences)\n",
    "    \n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        try:\n",
    "            ab = row['A'] if row['RA'] is None or len(row['RA']) == 0 else row['RA']\n",
    "            ch = row['M'] if row['RM'] is None or len(row['RM']) == 0 else row['RM']\n",
    "        except Exception as e:\n",
    "            print(x)\n",
    "            print(e)\n",
    "\n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "        \n",
    "        dff= pd.DataFrame({'Ab':[ab], 'Ch':[ch], 'Source': [f'{x.name[:4]}_{str(index)}']})\n",
    "    \n",
    "        results = pd.concat([results, dff], ignore_index = True)\n",
    "results = results.dropna()\n",
    "        \n",
    "display(results)\n",
    "results['From'] = '復興鄉故事二'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55438b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221226.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa353a02",
   "metadata": {},
   "source": [
    "# Egerod 1974 & Huang 1993 & Rau et al. 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221207.pkl', compression=\"gzip\")\n",
    "\n",
    "    \n",
    "results = []\n",
    "\n",
    "doc = Document(\"data/Egerod_1974_text.docx\")\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip()\n",
    "        results.append({\"Ab\": text})\n",
    "    elif para.text.startswith(\"e:\"):\n",
    "        text = para.text.lstrip(\"e:\").strip()\n",
    "        results[-1][\"Ch\"] = text   \n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Egerod (1974)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "\n",
    "# df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221219.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "doc = Document(\"data/Huang_1993_text.docx\")\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip()\n",
    "        results.append({\"Ab\": text})\n",
    "    elif para.text.startswith(\"e:\"):\n",
    "        text = para.text.lstrip(\"e:\").strip()\n",
    "        results[-1][\"Ch\"] = text   \n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Huang (1993)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "\n",
    "# df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221219.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "doc = Document(\"data/Rau_1995_text.docx\")\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip()\n",
    "        results.append({\"Ab\": text})\n",
    "    elif para.text.startswith(\"m:\"):\n",
    "        text = para.text.lstrip(\"m:\").strip()\n",
    "        results[-1][\"Ch\"] = text   \n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Rau et al. (1995)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "\n",
    "# df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221219.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53245e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221219.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5384c86",
   "metadata": {},
   "source": [
    "# Huang and Wu 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8882852",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "doc = Document(\"data/Huang&Wu_text_1_aligned.docx\")\n",
    "i = 0\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip().replace('\\xa0', ' ')\n",
    "        results.append({\"Ab\": text, \"Source\": f\"1_{i}\"})\n",
    "        i += 1\n",
    "    elif para.text.startswith(\"m:\"):\n",
    "        text = para.text.lstrip(\"m:\").strip()\n",
    "        results[-1][\"Ch\"] = text   \n",
    "        \n",
    "doc = Document(\"data/Huang&Wu_text_2_aligned.docx\")\n",
    "i = 0\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip().replace('\\xa0', ' ')\n",
    "        results.append({\"Ab\": text, \"Source\": f\"2_{i}\"})\n",
    "        i += 1\n",
    "    elif para.text.startswith(\"m:\"):\n",
    "        text = para.text.lstrip(\"m:\").strip()\n",
    "        results[-1][\"Ch\"] = text  \n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Huang & Wu (2016)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "results\n",
    "# df = pd.concat([df, results], ignore_index=True)\n",
    "\n",
    "# df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221219.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc929a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221226.pkl', compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 檢查各檔案的每個句子的tag('E:/clone/Formosan-languages/data/復興鄉故事2/F210_aligned.docx')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61051313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['From'] != '復興鄉故事二']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e099d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.From.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221226.pkl', compression=\"gzip\")\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221226-2.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b75194",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[df['From']=='復興鄉故事二']\n",
    "zz = x[x['Ab'].str.contains(\"mkayal ga\")]['Ab']\n",
    "pd.options.display.max_colwidth = 1000\n",
    "zz.to_string()\n",
    "display(zz)\n",
    "# type(zz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572cca7",
   "metadata": {},
   "source": [
    "# 重新整理傳說故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7128d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = pd.read_excel(\"data/Y&Y.xlsx\", sheet_name=None)\n",
    "\n",
    "results = []\n",
    "\n",
    "for story in yy:\n",
    "    story_id = story.strip()\n",
    "    for index, rows in yy[story].iterrows():\n",
    "        if isinstance(rows['Atayal'], float) or len(rows['Atayal'].strip()) == 0:\n",
    "            continue\n",
    "        atayal = rows['Atayal'].replace(' \\t', ' ').replace('\\t', '').strip()\n",
    "        ch = rows['Mandarin'].strip()\n",
    "#         if isinstance(ch, float):\n",
    "#             print(ch)\n",
    "#             print(index)\n",
    "#             print(rows)\n",
    "#             print(story_id)\n",
    "        results.append({\"Ab\": atayal, \"Source\": f\"{story_id}_{index}\", \"Ch\": ch})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "results['From'] = \"傳說故事精選篇\"\n",
    "\n",
    "# df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221226-2.pkl', compression=\"gzip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ba8a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['From'] != '傳說故事精選篇']\n",
    "df = pd.concat([df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33275b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rau1992 = pd.read_excel(\"data/Rau_1992_text_story.xlsx\", sheet_name=None)\n",
    "\n",
    "results = []\n",
    "\n",
    "for story in rau1992:\n",
    "    story_id = story.lstrip('0')\n",
    "    for index, rows in rau1992[story].iterrows():\n",
    "        if isinstance(rows['Atayal'], float) or len(rows['Atayal'].strip()) == 0:\n",
    "            continue\n",
    "        atayal = rows['Atayal'].replace(' \\t', ' ').replace('\\t', '').strip()\n",
    "        ch = rows['English'].strip()\n",
    "        results.append({\"Ab\": atayal, \"Ch\": ch, \"Source\": f\"{story_id}_{index}\"})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "results['From'] = \"Rau (1992)\"\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04813759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['From'] != \"Rau (1992)\"]\n",
    "df = pd.concat([df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "doc_id = 1\n",
    "for f in pathlib.Path(\"data/Rau_et_al_1995\").glob(\"Rau_et_al_1995_text_aligned_0*.docx\"):\n",
    "    print(f)\n",
    "    doc = Document(f)\n",
    "    phrase_id = 0\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.startswith(\"a:\"):\n",
    "            text = para.text.lstrip(\"a:\").strip()\n",
    "            results.append({\"Ab\": text, \"Source\": f\"{doc_id}_{phrase_id}\"})\n",
    "            phrase_id += 1\n",
    "        elif para.text.startswith(\"m:\"):\n",
    "            text = para.text.lstrip(\"m:\").strip()\n",
    "            results[-1][\"Ch\"] = text\n",
    "    doc_id += 1\n",
    "\n",
    "results\n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Rau et al. (1995)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8104b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['From'] != 'Rau et al. (1995)']\n",
    "df = pd.concat([df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51353dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221227.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa817f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_20221227.pkl', compression=\"gzip\")\n",
    "df = pd.concat([df, results], ignore_index=True)\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20221227-2.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb854e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94afdebc",
   "metadata": {},
   "source": [
    "# 將 Haowen 中其他語言的檔案也加進來 (2023-03-22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae152be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('./data/Formosan-Mandarin_sent_pairs_139023entries.pkl')\n",
    "df2 = pd.read_pickle('data/Formosan-Mandarin_sent_pairs_20221227-2.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa200649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[~df1['Lang_Ch'].str.contains('泰雅')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348cea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_27492\\3559124262.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df1.append(df2)\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19397417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.reset_index(drop=True)\n",
    "df3.to_pickle('./data/Formosan-Mandarin_sent_pairs_20230321.pkl', compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742b9d1f",
   "metadata": {},
   "source": [
    "# 移除 Huang 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14556074",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[~df3['From'].str.contains('Huang \\(1994\\)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d49961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.reset_index(drop=True)\n",
    "df3.to_pickle('./data/Formosan-Mandarin_sent_pairs_20230321.pkl', compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ce3072c",
   "metadata": {},
   "source": [
    "# 新增 Egerod 1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecda08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"data/Egerod_1969_text.docx\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    # print(para.text)\n",
    "    if para.text.startswith(\"a:\"):\n",
    "        text = para.text.lstrip(\"a:\").strip().replace('’', \"'\")\n",
    "        results.append({\"Ab\": text})\n",
    "    elif para.text.startswith(\"e:\"):\n",
    "        text = para.text.lstrip(\"e:\").strip()\n",
    "        results[-1][\"Ch\"] = text   \n",
    "# pprint(results)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results['From'] = 'Egerod (1969)'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n",
    "\n",
    "df = pd.concat([df3, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccef5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20230321.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b0248ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/Formosan-Mandarin_sent_pairs_20230321.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "208be951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141687, 6)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2980861f",
   "metadata": {},
   "source": [
    "# 更改復興鄉F106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d38b4655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 6)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df[df['Source'].isnull()]\n",
    "df_復興 = df[df['Source'].apply(str).str.contains('F106')]\n",
    "# df_復興 = df_復興[df_復興['Source'].isnull()]\n",
    "# df_復興 = df_復興[~df_復興['Source'].str.contains('106')]\n",
    "df_復興 = df[df['Source'].apply(str).str.contains('F106')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d3fab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141658, 6)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_復興 = df[~df['Source'].apply(str).str.contains('F106')]\n",
    "df_復興.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "647a5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([{'Ab':None, 'Ch':None, 'Source': None}])\n",
    "\n",
    "for x in pathlib.Path(\"data/復興鄉故事\").glob(\"F106*aligned.docx\"):\n",
    "    all_sentences = 檢查各檔案的每個句子的tag(x)\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "    all_sentences = all_sentences.replace({np.nan: None})\n",
    "    all_sentences = all_sentences.dropna(how='all')\n",
    "    # display(all_sentences)\n",
    "\n",
    "    for index, row in all_sentences.iterrows():\n",
    "        ab = \"\"\n",
    "        ch = \"\"\n",
    "        \n",
    "        try:\n",
    "            ab = row['A'] if row['RA'] is None or len(row['RA']) == 0 else row['RA']\n",
    "            ch = row['M'] if row['RM'] is None or len(row['RM']) == 0 else row['RM']\n",
    "        except Exception as e:\n",
    "            print(x)\n",
    "            print(e)\n",
    "\n",
    "        if ab is None or ch is None:\n",
    "            continue\n",
    "\n",
    "        if row['A'] is None and row['RA'] is None:\n",
    "            print('asdf')\n",
    "            continue\n",
    "        \n",
    "        df_= pd.DataFrame({'Ab':[ab], 'Ch':[ch], 'Source': [f'{x.name[:4]}_{str(index)}']})\n",
    "    #\n",
    "        results = pd.concat([results, df_], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "db358098",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['From'] = '復興鄉故事一'\n",
    "results['Lang_En'] = 'Atayal'\n",
    "results['Lang_Ch'] = '泰雅_賽考利克'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4aae9a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 6)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d6c00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_復興, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e283e2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141687, 6)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a0489341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/Formosan-Mandarin_sent_pairs_20230321.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bbe41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
